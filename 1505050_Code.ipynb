{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = bs(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(text):\n",
    "    text = strip_html(text)\n",
    "    \n",
    "    #Lowercase the text\n",
    "    text = text.lower()\n",
    "    #Number Removal\n",
    "    text = re.sub(r'[-+]?\\d+', '', text)\n",
    "    #Removing all slashes so that each work can be considered separately \n",
    "    text= text.replace(\"/\",\" \")\n",
    "    #Remove punctuations\n",
    "    text=text.translate((str.maketrans('','',string.punctuation)))\n",
    "    #Tokenize\n",
    "    text = word_tokenize(text)\n",
    "    #Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    #Lemmatize tokens\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    #Stemming tokens\n",
    "    stemmer= PorterStemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    \n",
    "    return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_union(list1, list2):\n",
    "\n",
    "    set_1 = set(list1)\n",
    "    set_2 = set(list2)\n",
    "\n",
    "    result_set = set_1.union(set_2)\n",
    "    result = list(result_set)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary d stores all the datas, where key is the topic name and value is a 2D list for preprocessed \n",
    "#words of each document\n",
    "# e.g. List[i] = list of strings of preprocessed words for row i\n",
    "d = {}\n",
    "\n",
    "# This list stores unique words from all training documents documents\n",
    "unique_words = []\n",
    "\n",
    "\n",
    "topic_file = open('Data/topics.txt')\n",
    "\n",
    "for each_line in topic_file:\n",
    "    #print(each_line)\n",
    "    topic_name = each_line.strip()\n",
    "    List = []\n",
    "\n",
    "    with open('Data/Training/'+topic_name+'.xml','r',encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        soup = bs(content)\n",
    "        \n",
    "        for id_no in range(1,1201):\n",
    "            \n",
    "            for items in soup.findAll(\"row\",id=id_no):\n",
    "                body = items.get('body')\n",
    "                body = preprocess_data(body)\n",
    "\n",
    "            List.append(body)\n",
    "            \n",
    "            if(id_no<=500):\n",
    "                unique_words = find_union(unique_words,body)\n",
    "\n",
    "            \n",
    "    d[topic_name]=List\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bool_representations = []   # boolean representations of all documents \n",
    "numeric_representations = [] # numeric representations of all documents\n",
    "\n",
    "for (x,y) in d.items():\n",
    "    topic_name = x\n",
    "\n",
    "    \n",
    "    for doc_no in range(0,len(y)):\n",
    "        document = y[doc_no]\n",
    "        \n",
    "        #csv_one_row = [0 for v in range(len(unique_words))]\n",
    "        bool_one_row = [0 for v in range(len(unique_words))]\n",
    "        numeric_one_row = [0 for v in range(len(unique_words))]\n",
    "        \n",
    "        for i in range(0,len(unique_words)):\n",
    "            temp1 = unique_words[i]\n",
    "\n",
    "            for j in range(0,len(document)):\n",
    "                temp2 = document[j]\n",
    "\n",
    "                if(temp1 == temp2):\n",
    "                    bool_one_row[i]=1\n",
    "                    numeric_one_row[i]=numeric_one_row[i]+1\n",
    "                    \n",
    "        bool_one_row.append(topic_name)\n",
    "        numeric_one_row.append(topic_name)\n",
    "        \n",
    "        bool_representations.append(bool_one_row)\n",
    "        numeric_representations.append(numeric_one_row)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime\n"
     ]
    }
   ],
   "source": [
    "# splitting into train, test and validation set\n",
    "training_set_bool = []\n",
    "validation_set_bool = []\n",
    "test_set_bool = []\n",
    "\n",
    "for i in range(0,len(bool_representations),1200):\n",
    "    #print(i)\n",
    "    for j in range(i,i+500):\n",
    "        training_set_bool.append(bool_representations[j])\n",
    "    for k in range(i+500,i+700):\n",
    "        validation_set_bool.append(bool_representations[k])\n",
    "    for l in range(i+700,i+1200):\n",
    "        test_set_bool.append(bool_representations[l])\n",
    "        \n",
    "\n",
    "training_set_numeric = []\n",
    "validation_set_numeric = []\n",
    "test_set_numeric = []\n",
    "\n",
    "for i in range(0,len(numeric_representations),1200):\n",
    "    #print(i)\n",
    "    for j in range(i,i+500):\n",
    "        training_set_numeric.append(numeric_representations[j])\n",
    "    for k in range(i+500,i+700):\n",
    "        validation_set_numeric.append(numeric_representations[k])\n",
    "    for l in range(i+700,i+1200):\n",
    "        test_set_numeric.append(numeric_representations[l])\n",
    "        \n",
    "print(training_set_numeric[1000][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here instance is 1D list, a row \n",
    "def HammingDistance(instance1, instance2):\n",
    "    distance = 0\n",
    "    for i in range(0,len(instance1)):\n",
    "        if(instance1[i]!=instance2[i]):\n",
    "            distance+=1\n",
    "            \n",
    "    return distance\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance(instance1, instance2):\n",
    "    distance = 0.0\n",
    "    for i in range(0,len(instance1)):\n",
    "        distance += (int(instance1[i]) - int(instance2[i]))**2\n",
    "    return np.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindAccuracy(actual, predicted):\n",
    "    total_count = len(predicted)\n",
    "    correct_count=0\n",
    "    \n",
    "    for i in range(0,len(predicted)):\n",
    "        if(actual[i]==predicted[i]):\n",
    "            correct_count+=1\n",
    "            \n",
    "    accuracy = (correct_count/total_count)*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainig set is the total training set, test document is only one document\n",
    "#containing the features only\n",
    "def Predict_document(train_X,train_Y,test_document_X,k):\n",
    "\n",
    "    #this dictionary keys are row index, values are distnace and class types \n",
    "    dist_type = {} \n",
    "    \n",
    "    for i in range(0,len(train_X)):\n",
    "        #distance = HammingDistance(train_X[i],test_document_X)\n",
    "        distance = EuclideanDistance(train_X[i],test_document_X)\n",
    "        \n",
    "        #distance = spatial.distance.cosine(train_X[i],test_document_X)\n",
    "        #distance = spatial.distance.hamming(train_X[i],test_document_X)\n",
    "        \n",
    "        #distance = spatial.distance.euclidean(train_X[i],test_document_X)\n",
    "        #print(distance)\n",
    "        dist_type[i]=(distance,train_Y[i])\n",
    "    \n",
    "    \n",
    "    #sort this dictionary according to distance values\n",
    "    dist_type = dict(sorted(dist_type.items(), key=lambda item: item[1][0]))\n",
    "    \n",
    "    #find the first k neighbors from the sorted dict\n",
    "    neighbors = dict(islice(dist_type.items(), k))\n",
    "    \n",
    "    neighbor_types=[]\n",
    "    for (x,y) in neighbors.items():\n",
    "        neighbor_types.append(y[1])\n",
    "\n",
    "    #find the class type by finding the class with maximum occurances\n",
    "    result = max(set(neighbor_types), key = neighbor_types.count)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here paramter should be a row from the training_set and test or validation\n",
    "def KNN(train_X,train_Y,test_X,test_Y,k):\n",
    "    \n",
    "\n",
    "    \n",
    "    #stores predicted results for all documents\n",
    "    all_predicted_outputs = []\n",
    "    \n",
    "    #epoch = 0\n",
    "    for testInput in test_X:\n",
    "        #print(epoch)\n",
    "        predicted_output = Predict_document(train_X, train_Y,testInput,k)\n",
    "        all_predicted_outputs.append(predicted_output)\n",
    "        \n",
    "        #epoch+=1\n",
    "        \n",
    "    \n",
    "   \n",
    "    acc = FindAccuracy(test_Y,all_predicted_outputs)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# k_val = 1\n",
    "\n",
    "# KNN(training_set_numeric,validation_set_numeric,k_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TF -IDF calculation starting\n",
    "\n",
    "def X_Y_split(dataset):\n",
    "    #index of the column containing class type\n",
    "    y_ind = len(dataset[0])-1\n",
    "    \n",
    "    X = np.delete(dataset,y_ind,axis=1)\n",
    "    Y = [row[y_ind] for row in dataset]\n",
    "    \n",
    "    return X,Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting feature and class\n",
    "train_numeric_X,train_Y = X_Y_split(training_set_numeric)\n",
    "train_numeric_X = [list( map(int,i) ) for i in train_numeric_X]\n",
    "\n",
    "validation_numeric_X,validation_Y = X_Y_split(validation_set_numeric)\n",
    "validation_numeric_X = [list( map(int,i) ) for i in validation_numeric_X]\n",
    "\n",
    "test_numeric_X,test_Y = X_Y_split(test_set_numeric)\n",
    "test_numeric_X = [list( map(int,i) ) for i in test_numeric_X]\n",
    "\n",
    "\n",
    "\n",
    "train_bool_X,train_Y = X_Y_split(training_set_bool)\n",
    "train_bool_X = [list( map(int,i) ) for i in train_bool_X]\n",
    "\n",
    "validation_bool_X,validation_Y = X_Y_split(validation_set_bool)\n",
    "validation_bool_X = [list( map(int,i) ) for i in validation_bool_X]\n",
    "\n",
    "test_bool_X,test_Y = X_Y_split(test_set_bool)\n",
    "test_bool_X = [list( map(int,i) ) for i in test_bool_X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_tf(dataset_X):\n",
    "    dataset_tf = []\n",
    "    length = len(dataset_X[0])  #since all documents have same number of columns\n",
    "    for i in range(0,len(dataset_X)):\n",
    "        doc = dataset_X[i]\n",
    "        doc_tf = []\n",
    "        total_w = sum(doc)\n",
    "        \n",
    "        if(total_w==0):\n",
    "            total_w = 1\n",
    "            \n",
    "        for j in range(0,length):\n",
    "            val = doc[j]\n",
    "            tf= val/total_w\n",
    "            doc_tf.append(tf)\n",
    "            \n",
    "        dataset_tf.append(doc_tf)\n",
    "    \n",
    "    return dataset_tf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_idf(dataset_X):\n",
    "    IDF = []\n",
    "    D = len(dataset_X)\n",
    "    column_as_row = list(zip(*dataset_X))\n",
    "    \n",
    "    for i in range(0,len(column_as_row)):\n",
    "        d = np.count_nonzero(column_as_row[i])\n",
    "        val = math.log(D/d)\n",
    "        \n",
    "        IDF.append(val)\n",
    "        \n",
    "    return IDF\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_values is 2d list, idf_values is 1D list\n",
    "def finding_tf_idf(tf_values,idf_values):\n",
    "    tf_idf = []\n",
    "    for row in tf_values:\n",
    "        temp = np.multiply(row,idf_values)\n",
    "        tf_idf.append(temp)\n",
    "        \n",
    "    return tf_idf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting tf and idf separately\n",
    "training_tf = finding_tf(train_numeric_X)\n",
    "validation_tf = finding_tf(validation_numeric_X)\n",
    "test_tf = finding_tf(test_numeric_X)\n",
    "\n",
    "IDF_for_unique_words= finding_idf(train_numeric_X) #calculated on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tf_idf = finding_tf_idf(training_tf,IDF_for_unique_words)\n",
    "validation_tf_idf = finding_tf_idf(validation_tf,IDF_for_unique_words)\n",
    "test_tf_idf = finding_tf_idf(test_tf,IDF_for_unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "### KNN function is used for all of the three distances cases.\n",
    "### Just change the distance calculation in function Predict_document\n",
    "\n",
    "\n",
    "#print(KNN(training_tf_idf,train_Y, validation_tf_idf,validation_Y,1))\n",
    "print(KNN(train_bool_X,train_Y, validation_bool_X,validation_Y,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the report , best KNN is for Euclidean distance and k=1\n",
    "### This is for KNN\n",
    "## this one iteration takes 10 documents form test set of each topic\n",
    "\n",
    "def RunOneItrn(train_X,train_Y,test_X,test_Y,k,ind):\n",
    "    temp_test_X = []\n",
    "    temp_test_Y =[]\n",
    "    \n",
    "    for i in range(0,len(test_X),500):\n",
    "        for j in range(i+ind,i+ind+10):\n",
    "            temp_test_X.append(test_X[j])\n",
    "            temp_test_Y.append(test_Y[j])\n",
    "            \n",
    "    result=KNN(train_X,train_Y,temp_test_X,temp_test_Y,k)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RunOneItrn(train_numeric_X,train_Y,test_numeric_X,test_Y,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: 60.0\n",
      "\n",
      "Iteration 2: 60.0\n",
      "\n",
      "Iteration 3: 76.66666666666667\n",
      "\n",
      "Iteration 4: 53.333333333333336\n",
      "\n",
      "Iteration 5: 63.33333333333333\n",
      "\n",
      "Iteration 6: 60.0\n",
      "\n",
      "Iteration 7: 66.66666666666666\n",
      "\n",
      "Iteration 8: 60.0\n",
      "\n",
      "Iteration 9: 50.0\n",
      "\n",
      "Iteration 10: 66.66666666666666\n",
      "\n",
      "Iteration 11: 56.666666666666664\n",
      "\n",
      "Iteration 12: 66.66666666666666\n",
      "\n",
      "Iteration 13: 66.66666666666666\n",
      "\n",
      "Iteration 14: 43.333333333333336\n",
      "\n",
      "Iteration 15: 76.66666666666667\n",
      "\n",
      "Iteration 16: 66.66666666666666\n",
      "\n",
      "Iteration 17: 63.33333333333333\n",
      "\n",
      "Iteration 18: 70.0\n",
      "\n",
      "Iteration 19: 70.0\n",
      "\n",
      "Iteration 20: 63.33333333333333\n",
      "\n",
      "Iteration 21: 63.33333333333333\n",
      "\n",
      "Iteration 22: 66.66666666666666\n",
      "\n",
      "Iteration 23: 73.33333333333333\n",
      "\n",
      "Iteration 24: 83.33333333333334\n",
      "\n",
      "Iteration 25: 83.33333333333334\n",
      "\n",
      "Iteration 26: 96.66666666666667\n",
      "\n",
      "Iteration 27: 90.0\n",
      "\n",
      "Iteration 28: 90.0\n",
      "\n",
      "Iteration 29: 76.66666666666667\n",
      "\n",
      "Iteration 30: 80.0\n",
      "\n",
      "Iteration 31: 93.33333333333333\n",
      "\n",
      "Iteration 32: 70.0\n",
      "\n",
      "Iteration 33: 80.0\n",
      "\n",
      "Iteration 34: 83.33333333333334\n",
      "\n",
      "Iteration 35: 86.66666666666667\n",
      "\n",
      "Iteration 36: 80.0\n",
      "\n",
      "Iteration 37: 90.0\n",
      "\n",
      "Iteration 38: 93.33333333333333\n",
      "\n",
      "Iteration 39: 86.66666666666667\n",
      "\n",
      "Iteration 40: 83.33333333333334\n",
      "\n",
      "Iteration 41: 80.0\n",
      "\n",
      "Iteration 42: 93.33333333333333\n",
      "\n",
      "Iteration 43: 86.66666666666667\n",
      "\n",
      "Iteration 44: 90.0\n",
      "\n",
      "Iteration 45: 93.33333333333333\n",
      "\n",
      "Iteration 46: 86.66666666666667\n",
      "\n",
      "Iteration 47: 96.66666666666667\n",
      "\n",
      "Iteration 48: 80.0\n",
      "\n",
      "Iteration 49: 93.33333333333333\n",
      "\n",
      "Iteration 50: 86.66666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "KNN_file = open(\"KNN_result.txt\",\"w\")\n",
    "KNN_file.write('KNN accuracy result for 50 iterations over test set.\\nEuclidean Distance and k=1\\n\\n')\n",
    "\n",
    "for i in range(0,50):\n",
    "    index = i*10\n",
    "    accuracy = RunOneItrn(train_numeric_X,train_Y,test_numeric_X,test_Y,1,index)\n",
    "    line = 'Iteration '+str(i+1)+': '+str(accuracy)+'\\n'\n",
    "    print(line)\n",
    "    KNN_file.write(line)\n",
    "\n",
    "KNN_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Coffee', 1: 'Arduino', 2: 'Anime'}\n"
     ]
    }
   ],
   "source": [
    "### Naive Bayes Implementation\n",
    "\n",
    "## To simplify calculation, this dictionary is introduced.\n",
    "## dictionary value is the class type \n",
    "\n",
    "class_value_dict = {}\n",
    "\n",
    "j = 0\n",
    "for i in range(0,len(training_set_numeric),500):\n",
    "    class_value_dict[j]=training_set_numeric[i][-1]\n",
    "    j+=1\n",
    "    \n",
    "#print(class_value_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#which is NCk on slide\n",
    "def find_total_words(dataset):\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(0,len(dataset)):\n",
    "        count+=sum(dataset[i])\n",
    "        \n",
    "    return count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is Nwi,ck on slide. This counts each word occurances\n",
    "def find_occurances_of_words(dataset):\n",
    "    occurance_count=[]\n",
    "    \n",
    "    column_as_row = list(zip(*dataset))\n",
    "    \n",
    "    for i in range(0,len(column_as_row)):\n",
    "        occurance_count.append(sum(column_as_row[i]))\n",
    "        \n",
    "    return occurance_count\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is V in slide\n",
    "def total_diff_words(dataset):\n",
    "    count=0\n",
    "    column_as_row = list(zip(*dataset))\n",
    "    \n",
    "    for i in range(0,len(column_as_row)):\n",
    "        temp = np.count_nonzero(column_as_row[i])\n",
    "        if(temp!=0):\n",
    "            count+=1\n",
    "            \n",
    "    return count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_wise_total_words=[]  #1D list \n",
    "class_wise_occurances_of_words= [] #2D list, row is the class number\n",
    "class_wise_total_diff_words=[] #1D list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "### finding on all training documents\n",
    "for i in range(0,len(train_numeric_X),500):\n",
    "    one_class_documents=[]\n",
    "    for j in range(i,i+500):\n",
    "        one_class_documents.append(train_numeric_X[j])\n",
    "        \n",
    "    class_wise_total_words.append(find_total_words(one_class_documents))\n",
    "    class_wise_occurances_of_words.append(find_occurances_of_words(one_class_documents))\n",
    "    class_wise_total_diff_words.append(total_diff_words(one_class_documents))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_a_doc_NB(test_doc,total_words,occur_words,diff_words,class_val_dict,alpha):\n",
    "    class_count= len(occur_words)\n",
    "    \n",
    "    prob_all_classes=[]\n",
    "    \n",
    "    for i in range(0,class_count):\n",
    "        prob_this_class = 0\n",
    "        total_w = total_words[i] #single value\n",
    "        total_diff_w = diff_words[i] #single value\n",
    "        occurances_w= occur_words[i]  #its a list\n",
    "        \n",
    "        for j in range(0,len(test_doc)):\n",
    "            \n",
    "            if(test_doc[j]!=0):\n",
    "                p = (occurances_w[j]+alpha)/(total_w + alpha*total_diff_w)\n",
    "                prob_this_class+=p\n",
    "                \n",
    "        prob_all_classes.append(prob_this_class)\n",
    "        \n",
    "    max_index = prob_all_classes.index(max(prob_all_classes))\n",
    "    \n",
    "    predicted_class = class_val_dict[max_index]\n",
    "    \n",
    "    return predicted_class\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(test_X,test_Y,total_words,occur_words,diff_words,class_val_dict,alpha):\n",
    "\n",
    "    \n",
    "    #stores predicted results for all documents\n",
    "    all_predicted_outputs = []\n",
    "    \n",
    "    epoch = 0\n",
    "    for testInput in test_X:\n",
    "        #print(epoch)\n",
    "        predicted_output = predict_a_doc_NB(testInput,total_words,occur_words,diff_words,class_val_dict,alpha)\n",
    "        all_predicted_outputs.append(predicted_output)\n",
    "        \n",
    "        epoch+=1\n",
    "\n",
    "    acc = FindAccuracy(test_Y,all_predicted_outputs)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.83333333333333"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NB(validation_numeric_X,validation_Y,class_wise_total_words,class_wise_occurances_of_words,class_wise_total_diff_words,class_value_dict,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha  0.1  acc : 92.66666666666666\n",
      "alpha  0.2  acc : 92.33333333333333\n",
      "alpha  0.30000000000000004  acc : 92.33333333333333\n",
      "alpha  0.4  acc : 92.33333333333333\n",
      "alpha  0.5  acc : 92.16666666666666\n",
      "alpha  0.6  acc : 92.16666666666666\n",
      "alpha  0.7  acc : 92.16666666666666\n",
      "alpha  0.7999999999999999  acc : 92.16666666666666\n",
      "alpha  0.8999999999999999  acc : 92.16666666666666\n",
      "alpha  0.9999999999999999  acc : 92.0\n"
     ]
    }
   ],
   "source": [
    "# NB on validation set for 10 diff alpha\n",
    "alpha = 0.1\n",
    "for i in range(0,10):\n",
    "    res=NB(validation_numeric_X,validation_Y,class_wise_total_words,class_wise_occurances_of_words,class_wise_total_diff_words,class_value_dict,alpha)\n",
    "    print('alpha ',alpha,' acc :',res)\n",
    "    alpha+=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From the report , best NB for smoothing factor 0.005\n",
    "# this one iteration takes 10 documents form test set of each topic\n",
    "\n",
    "\n",
    "def RunOneItrn_NB(test_X,test_Y,total_words,occur_words,diff_words,class_val_dict,alpha,ind):\n",
    "    \n",
    "    temp_test_X = []\n",
    "    temp_test_Y =[]\n",
    "    \n",
    "    for i in range(0,len(test_X),500):\n",
    "        \n",
    "        for j in range(i+ind,i+ind+10):\n",
    "            temp_test_X.append(test_X[j])\n",
    "            temp_test_Y.append(test_Y[j])\n",
    "            \n",
    "    result = NB(temp_test_X,temp_test_Y,total_words,occur_words,diff_words,class_val_dict,alpha)\n",
    "    return result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_file = open(\"NB_result.txt\",\"w\")\n",
    "NB_file.write('NB accuracy result for 50 iterations over test set.\\nSmoothing factor is 0.005\\n\\n')\n",
    "\n",
    "for i in range(0,50):\n",
    "    ind = i*10\n",
    "    \n",
    "    accuracy = RunOneItrn_NB(test_numeric_X,test_Y,class_wise_total_words,class_wise_occurances_of_words,class_wise_total_diff_words,class_value_dict,alpha,ind)\n",
    "    line = 'Iteration '+str(i+1)+': '+str(accuracy)+'\\n'\n",
    "    #print(line)\n",
    "    NB_file.write(line)\n",
    "\n",
    "NB_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### t- statistics calculation\n",
    "\n",
    "file1 = open('KNN_result.txt','r')\n",
    "KNN_result_for_test=[]\n",
    "\n",
    "for each_line in file1 :\n",
    "    each_line = each_line.strip()\n",
    "    \n",
    "    stream = each_line.split()\n",
    "    KNN_result_for_test.append(float(stream[-1]))\n",
    "\n",
    "#print(KNN_result_for_test)\n",
    "\n",
    "\n",
    "file2 = open('NB_result.txt','r')\n",
    "NB_result_for_test=[]\n",
    "\n",
    "for each_line in file2 :\n",
    "    each_line = each_line.strip()\n",
    "    \n",
    "    stream = each_line.split()\n",
    "    NB_result_for_test.append(float(stream[-1]))\n",
    "\n",
    "#print(NB_result_for_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_relResult(statistic=0.7906173382529886, pvalue=0.4329773247829769)\n"
     ]
    }
   ],
   "source": [
    "# Here ttest_rel is used, since test set is same for two impelementations\n",
    "\n",
    "print(stats.ttest_rel(NB_result_for_test,KNN_result_for_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
